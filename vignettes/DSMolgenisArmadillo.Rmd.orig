---
title: "Analyse your data"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Analyse your data}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE, comment = "#>", out.width = "100%",
  fig.width = 7, fig.height = 4, dpi = 150, fig.path = "DSMolgenisArmadillo-",
  message = FALSE, warning = FALSE, error = FALSE
)
```

When you start to use Armadillo as a backend for DataSHIELD you can use the `DSMolgenisArmadillo` package for research purposes.
There is a default workflow in DataSHIELD to do analysis. These are the steps that you need to take:

### Authenticate
First obtain a token from the authentication server to authenticate in DataSHIELD.

```{r, obtain token}
# Load the necessary packages.
library(dsBaseClient)
library(DSMolgenisArmadillo)

# specify server url
armadillo_url <- "https://armadillo-demo.molgenis.net"

# get token from central authentication server
token <- armadillo.get_token(armadillo_url)
```

Then build a login dataframe and perform the login on the Armadillo server.
The important part is to specify the driver. This should be `ArmadilloDriver`.
```{r, login}
# build the login dataframe
builder <- DSI::newDSLoginBuilder()
builder$append(
  server = "armadillo",
  url = armadillo_url,
  token = token,
  driver = "ArmadilloDriver",
  profile = "xenon",
  )

# create loginframe
login_data <- builder$build()

# login into server
conns <- DSI::datashield.login(logins = login_data, assign = FALSE)
```

> You can append multiple servers to the login frame to perform an analysis
across multiple cohorts.

### Assign data
To work with DataSHIELD you need to be able to query data.
You can do this by assigning data in the Armadillo service.

#### Assign data using expressions
You can assign values from expressions to symbols.
```{r, assign some data to K}
# assign some data to 'K'
datashield.assign.expr(conns = conns, symbol = "K", "c(10,50,100)")
```

```{r, calculate the mean}
# calculate the mean of 'K' to see that the assignment has worked
ds.mean("K", datasources = conns)
```

#### Assign data from tables
You can check which tables (`data.frame`'s) are available on the Armadillo.

```{r, list tables}
datashield.tables(conns)
```

And load data from one of these tables.
```{r, assign data using the table assignment method}
# assign table data to a symbol
datashield.assign.table(
  conns = conns,
  table = "gecko/2_1-core-1_0/nonrep",
  symbol = "core_nonrep"
)
```

```{r, show columns within the data frame}
# check the columns in the non-repeated data
ds.colnames("core_nonrep", datasources = conns)
```

#### Assign data at login time
You can also specify a table in the login frame and assign the data when you
login.

```{r, assign data during login}
# build the login dataframe
builder <- DSI::newDSLoginBuilder()
builder$append(
  server = "armadillo",
  url = armadillo_url,
  token = token,
  driver = "ArmadilloDriver",
  table = "gecko/2_1-core-1_0/nonrep",
  profile = "xenon",
  )

# create loginframe
login_data <- builder$build()

# login into server
conns <- DSI::datashield.login(logins = login_data, assign = TRUE, symbol="core_nonrep")
```

### Subsetting data
Before you are working with the data you can subset to a specific range of
variables you want to use in the set.

```{r, subset the data to the first 3 years}
# assign the repeated data to reshape
datashield.assign.table(
  conns = conns,
  table = "gecko/2_1-core-1_0/yearlyrep",
  symbol = "core_yearlyrep"
)

# check dimensions of repeatead measures
ds.dim("core_yearlyrep", datasources = conns)

# subset the data to the first 2 years
ds.dataFrameSubset(
  df.name = "core_yearlyrep",
  newobj = "core_yearlyrep_1_3",
  V1.name = "core_yearlyrep$age_years",
  V2.name = "2",
  Boolean.operator = "<="
)

# check the columns
ds.colnames("core_yearlyrep_1_3", datasources = conns)

# check dimensions again
ds.dim("core_yearlyrep_1_3", datasources = conns)
```

```{r, strip the redundant columns}
# strip the redundant columns
ds.dataFrame(
  x = c("core_yearlyrep_1_3$child_id",
        "core_yearlyrep_1_3$age_years",
        "core_yearlyrep_1_3$dogs_",
        "core_yearlyrep_1_3$cats_",
        "core_yearlyrep_1_3$pets_"),
  completeCases = TRUE,
  newobj = "core_yearlyrep_1_3_stripped",
  datasources = conns
)
```

### Transform data
In general you need 2 methods to work with data that is stored in long format,
the `reshape` and `merge` functions in DataSHIELD. You can reshape data with
the Armadillo to transform data from [wide-format to long-format](https://www.theanalysisfactor.com/wide-and-long-data/) and
vice versa.

You can do this using the `ds.reshape` function:

```{r, reshape yearly repeated data}
# reshape the data for the wide-format variables (yearlyrep)
ds.reShape(
  data.name = "core_yearlyrep_1_3_stripped",
  timevar.name = "age_years",
  idvar.name = "child_id",
  v.names = c("pets_", "cats_", "dogs_"),
  direction = "wide",
  newobj = "core_yearlyrep_1_3_wide",
  datasources = conns
)
```

```{r, show columnnames from wide format data}
# show the reshaped columns of the new data frame
ds.colnames("core_yearlyrep_1_3_wide", datasources = conns)
```

When you reshaped and subsetted the data you often need to merge your dataframe
with others to get your analysis dataframe. You can do this using the `ds.merge`
function:

```{r, merge both data frames into one}
# merge non-repeated table with wide-format repeated table
# make sure the disclosure measure regarding stringshort is set to '100'
ds.merge(
  x.name = "core_nonrep",
  y.name = "core_yearlyrep_1_3_wide",
  by.x.names = "child_id",
  by.y.names = "child_id",
  newobj = "analysis_df",
  datasources = conns
)
```

```{r, check the merged variables}
ds.colnames("analysis_df", datasources = conns)
```

### Save your work
When you finished building your analysis frame you can save it using
[workspaces](workspaces.html).

### Performing analysis
There are a variety of analysis you can perform in DataSHIELD. You can perform
basic methods such as summary statistics and more advanced methods such as GLM.

#### Simple statistical methods
You execute a summary on the a variable within you analysis frame. It will
return summary statistics.

```{r, summarize the data}
ds.summary("analysis_df$pets_.1", datasources = conns)
```

#### Advanced statistical methods
When you finished the analysis dataframe, you can perform the actual analysis.
You can use a wide variety of functions. The example below is showing the `glm`.

```{r, perform the GLM method}
datashield.assign.table(
  conns = conns,
  table = "gecko/1_1-outcome-1_0/nonrep",
  symbol = "outcome_nonrep"
)

armadillo_glm <- ds.glm(
  formula = "asthma_ever_CHICOS~pets_preg",
  data = "outcome_nonrep",
  family = "binomial",
  datasources = conns
)
```

Do the meta analysis and install prerequisites.

```{r, install prerequisites for meta analysis}
if (!require('metafor')) install.packages('metafor')
```

```{r, meta-analysis}
yi <- c(armadillo_glm$coefficients["pets_preg", "Estimate"])
sei <- c(armadillo_glm$coefficients["pets_preg", "Std. Error"])

res <- metafor::rma(yi, sei = sei)
res
metafor::forest(res, xlab = "OR", transf = exp, refline = 1, slab = c("armadillo_glm"))
```

#### Creating figures
You can directly create figures with the DataSHIELD methods. For example:

```{r, create-a-histogram}
# create histogram
ds.histogram(x = "core_nonrep$coh_country", datasources = conns)
```


```{r, create-a-heatmap}
# create a heatmap
ds.heatmapPlot(x = "analysis_df$pets_.1", y = "analysis_df$dogs_.1", datasources = conns)
```

```{r, logout to clean session}
# logout
datashield.logout(conns)
```
